{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37daddb6-a110-44cb-a830-d74f6cd9d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edae0c40-6405-485c-987d-2542af23f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c011737f-e163-437c-8027-56797e6e6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my* *의 의미는 앞에 my만 같으면 다불러온다.\n",
    "all_files = glob.glob(\"./data_list/my*.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d74d16b-2d8c-4270-bad7-3cbc030befd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86745380-dc31-4ce3-976d-88b0fa090fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_date=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c6ff1-1aba-4ab7-b433-6e796add8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in all_files:\n",
    "    data_frame =pd.read_execl(file)\n",
    "    all_files_data.append(data_frame)\n",
    "all_files_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325443c9-5be2-41a6-a8f8-3b6366145895",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data_concat=pd.concat(all_files_data,axis=0,ignore_index=True)\n",
    "all_files_data_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b3eb6-d527-4a0f-bda6-28dbd555b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_title = all_files_data_concat['제목']\n",
    "all_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df2875-576c-42e2-8850-fd7c167fb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator data : 반복되는 형태의 데이터 list, series, tuple  (반복자)\n",
    "type(all_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3400a-9f7d-4a2c-85e2-abe86413e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords= set(stopwords.words('english'))\n",
    "lemma= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7ac9d-54e1-47ce-a3e4-cb51f46ccbd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words= []\n",
    "for title in all_title:\n",
    "    # print(title)\n",
    "    EnWords =re.sub(r\"[^a-zA-Z]+\",\" \",str(title)) # [^a-zA-Z] 소문자부터 대문자까지 다포함해라\n",
    "    # print(EnWords)\n",
    "    EnWordsToken = word_tokenize(EnWords.lower()) # 여기서 word_tokenize로 리스트가 되기때문에 밑에도 리스트를해줘야한다.\n",
    "    # print(EnWordsToken)\n",
    "    EnWordsTokenStop = [W for w in EnWordsToken if w not in stopWords] # 이름이 없는 함수 익명함수,클로저\n",
    "    # print(EnWordsTokenStop)\n",
    "    EnWordsTokenStopLemma= [lemma.lemmatize(w) for w in EnWordsTokenStop]\n",
    "    # print(EnWordsTokenStopLemma)\n",
    "    words.append(EnWordsTokenStopLemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafff73-7be1-4d8d-bf04-39abf9a1764e",
   "metadata": {},
   "source": [
    "A =\n",
    "EnWordsTokenStop = [W for w in EnWordsToken if w not in stopWords]\n",
    "\n",
    "A코드와 \n",
    "B코드는 같은 코드\n",
    "A 메모리에 저장이안되고 바로쓰고 버리고\n",
    "B 메모리에남기때문에 굳이 B 보단 A 가 메모리적으론 좋다 \n",
    "B 방법이 익숙하다면 써도 상관무\n",
    "\n",
    "B=\n",
    "def call(EnWordsTocken):\n",
    "    result = []\n",
    "    for w in EnWordsToken:\n",
    "        if w not in stopwords:\n",
    "            result.append(w)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6e55a4-cb5e-40e6-b105-20e25d844c7c",
   "metadata": {},
   "source": [
    "EnWordsTokenStopLemma= [lemma.lemmatize(w) for w in EnWordsTokenStop]\n",
    "같은 코드 \n",
    "def call(EnWordsTokenStop):\n",
    "    result2 = []\n",
    "    for w in EnWordsTokenStop:\n",
    "         result2.append(lemma.lematize(w))     \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2584a85-9358-4095-969d-ff5b40bd86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
